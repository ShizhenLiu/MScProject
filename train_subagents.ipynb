{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install MineRL v1.0"
      ],
      "metadata": {
        "id": "FE429SaN6o89"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC35vlHK6kir"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!sudo add-apt-repository -y ppa:openjdk-r/ppa\n",
        "!sudo apt-get purge openjdk-*\n",
        "!sudo apt-get install openjdk-8-jdk\n",
        "!sudo apt-get install xvfb\n",
        "!sudo apt-get install xserver-xephyr\n",
        "!sudo apt-get install vnc4server\n",
        "!sudo apt-get install python-opengl\n",
        "!sudo apt-get install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip3 install pyvirtualdisplay\n",
        "!pip3 install -U colabgymrender\n",
        "!pip3 install imageio==2.4.1"
      ],
      "metadata": {
        "id": "GaVLBhvQ6-MR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install git+https://github.com/minerllabs/minerl"
      ],
      "metadata": {
        "id": "qvoVXQXJ6-ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install MineRL v4.0(datasets)"
      ],
      "metadata": {
        "id": "85x2ZcAu63UE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!sudo add-apt-repository -y ppa:openjdk-r/ppa\n",
        "!sudo apt-get purge openjdk-*\n",
        "!sudo apt-get install openjdk-8-jdk\n",
        "!sudo apt-get install xvfb\n",
        "!sudo apt-get install xserver-xephyr\n",
        "!sudo apt-get install vnc4server\n",
        "!sudo apt-get install python-opengl\n",
        "!sudo apt-get install ffmpeg"
      ],
      "metadata": {
        "id": "q4jHgVfD7AOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip3 install pyvirtualdisplay\n",
        "!pip3 install -U colabgymrender\n",
        "!pip3 install imageio==2.4.1"
      ],
      "metadata": {
        "id": "eBWOyEUrC-Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!python -m pip install --upgrade pip wheel==0.38.4 setuptools==65.6.1"
      ],
      "metadata": {
        "id": "Iy_6c5TiC-zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install minerl==0.4.4"
      ],
      "metadata": {
        "id": "d6xuoF-2C_cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "2QaNDu8yDBBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import gym\n",
        "import minerl\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from colabgymrender.recorder import Recorder\n",
        "from pyvirtualdisplay import Display\n",
        "import logging\n",
        "logging.disable(logging.ERROR)\n",
        "import cv2\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "np.__version__"
      ],
      "metadata": {
        "id": "5nUMghK7DDaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disp = Display(visible=0, backend=\"xvfb\")\n",
        "disp.start();"
      ],
      "metadata": {
        "id": "aoujJezMDEaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scenario"
      ],
      "metadata": {
        "id": "vaFJGaQdDK5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from minerl.herobraine.env_specs.human_controls import SimpleHumanEmbodimentEnvSpec\n",
        "from minerl.herobraine.hero.mc import MS_PER_STEP, STEPS_PER_MS\n",
        "from minerl.herobraine.hero.handler import Handler\n",
        "from typing import List\n",
        "\n",
        "import minerl.herobraine\n",
        "import minerl.herobraine.hero.handlers as handlers\n",
        "from minerl.herobraine.env_spec import EnvSpec\n",
        "from minerl.herobraine.hero.mc import MS_PER_STEP, STEPS_PER_MS, ALL_ITEMS\n",
        "from minerl.herobraine.hero import handlers as H, mc\n",
        "from minerl.herobraine.hero.handlers.translation import TranslationHandler\n",
        "\n",
        "MY_TREECHOP_DOC = \"\"\"\n",
        "In treechop, the agent must collect 64 `minercaft:log`. This replicates a common scenario in Minecraft, as logs are necessary to craft a large amount of items in the game, and are a key resource in Minecraft.\n",
        "\n",
        "The agent begins in a forest biome (near many trees) with an iron axe for cutting trees. The agent is given +1 reward for obtaining each unit of wood, and the episode terminates once the agent obtains 64 units.\n",
        "\"\"\"\n",
        "TREECHOP_LENGTH = 8000\n",
        "TREECHOP_WORLD_GENERATOR_OPTIONS = \"\"\"{\"coordinateScale\":684.412,\"heightScale\":684.412,\"lowerLimitScale\":512.0,\"upperLimitScale\":512.0,\"depthNoiseScaleX\":200.0,\"depthNoiseScaleZ\":200.0,\"depthNoiseScaleExponent\":0.5,\"mainNoiseScaleX\":80.0,\"mainNoiseScaleY\":160.0,\"mainNoiseScaleZ\":80.0,\"baseSize\":8.5,\"stretchY\":12.0,\"biomeDepthWeight\":1.0,\"biomeDepthOffset\":0.0,\"biomeScaleWeight\":1.0,\"biomeScaleOffset\":0.0,\"seaLevel\":1,\"useCaves\":false,\"useDungeons\":false,\"dungeonChance\":8,\"useStrongholds\":false,\"useVillages\":false,\"useMineShafts\":false,\"useTemples\":false,\"useMonuments\":false,\"useMansions\":false,\"useRavines\":false,\"useWaterLakes\":false,\"waterLakeChance\":4,\"useLavaLakes\":false,\"lavaLakeChance\":80,\"useLavaOceans\":false,\"fixedBiome\":4,\"biomeSize\":4,\"riverSize\":1,\"dirtSize\":33,\"dirtCount\":10,\"dirtMinHeight\":0,\"dirtMaxHeight\":256,\"gravelSize\":33,\"gravelCount\":8,\"gravelMinHeight\":0,\"gravelMaxHeight\":256,\"graniteSize\":33,\"graniteCount\":10,\"graniteMinHeight\":0,\"graniteMaxHeight\":80,\"dioriteSize\":33,\"dioriteCount\":10,\"dioriteMinHeight\":0,\"dioriteMaxHeight\":80,\"andesiteSize\":33,\"andesiteCount\":10,\"andesiteMinHeight\":0,\"andesiteMaxHeight\":80,\"coalSize\":17,\"coalCount\":20,\"coalMinHeight\":0,\"coalMaxHeight\":128,\"ironSize\":9,\"ironCount\":20,\"ironMinHeight\":0,\"ironMaxHeight\":64,\"goldSize\":9,\"goldCount\":2,\"goldMinHeight\":0,\"goldMaxHeight\":32,\"redstoneSize\":8,\"redstoneCount\":8,\"redstoneMinHeight\":0,\"redstoneMaxHeight\":16,\"diamondSize\":8,\"diamondCount\":1,\"diamondMinHeight\":0,\"diamondMaxHeight\":16,\"lapisSize\":7,\"lapisCount\":1,\"lapisCenterHeight\":16,\"lapisSpread\":16}\"\"\"\n",
        "\n",
        "\n",
        "class MyTreechop(SimpleHumanEmbodimentEnvSpec):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        if 'name' not in kwargs:\n",
        "            kwargs['name'] = 'MyMineRLTreechop-v0'\n",
        "\n",
        "        super().__init__(*args,\n",
        "                         max_episode_steps=TREECHOP_LENGTH, reward_threshold=64.0,resolution=[640, 360],\n",
        "                         **kwargs)\n",
        "\n",
        "    def create_observables(self) -> List[Handler]:\n",
        "        return super().create_observables() + [\n",
        "            handlers.EquippedItemObservation(\n",
        "                items=ALL_ITEMS,\n",
        "                mainhand=True,\n",
        "                offhand=True,\n",
        "                armor=True,\n",
        "                _default=\"air\",\n",
        "                _other=\"air\",\n",
        "            ),\n",
        "            handlers.ObservationFromLifeStats(),\n",
        "            handlers.ObservationFromCurrentLocation(),\n",
        "            handlers.ObserveFromFullStats(\"use_item\"),\n",
        "            handlers.ObserveFromFullStats(\"drop\"),\n",
        "            handlers.ObserveFromFullStats(\"pickup\"),\n",
        "            handlers.ObserveFromFullStats(\"break_item\"),\n",
        "            handlers.ObserveFromFullStats(\"craft_item\"),\n",
        "            handlers.ObserveFromFullStats(\"mine_block\"),\n",
        "            handlers.ObserveFromFullStats(\"damage_dealt\"),\n",
        "            handlers.ObserveFromFullStats(\"entity_killed_by\"),\n",
        "            handlers.ObserveFromFullStats(\"kill_entity\"),\n",
        "            handlers.FlatInventoryObservation(ALL_ITEMS),\n",
        "            # handlers.ObserveFromFullStats(None),\n",
        "        ]\n",
        "\n",
        "    def create_actionables(self) -> List[TranslationHandler]:\n",
        "        \"\"\"\n",
        "        Simple envs have some basic keyboard control functionality, but\n",
        "        not all.\n",
        "        \"\"\"\n",
        "        return [\n",
        "           H.KeybasedCommandAction(v, v) for v in mc.KEYMAP.values()\n",
        "        ] + [H.CameraAction()]\n",
        "\n",
        "    def create_rewardables(self) -> List[Handler]:\n",
        "        return [\n",
        "            handlers.RewardForCollectingItems([\n",
        "                dict(type=\"log\", amount=1, reward=1.0),\n",
        "            ])\n",
        "        ]\n",
        "\n",
        "    def create_agent_start(self) -> List[Handler]:\n",
        "        return super().create_agent_start() + [\n",
        "            handlers.SimpleInventoryAgentStart([\n",
        "                dict(type=\"oak_log\", quantity=4)\n",
        "            ])\n",
        "        ]\n",
        "\n",
        "    def create_agent_handlers(self) -> List[Handler]:\n",
        "        return [\n",
        "            handlers.AgentQuitFromPossessingItem([\n",
        "                dict(type=\"log\", amount=64)]\n",
        "            )\n",
        "        ]\n",
        "\n",
        "    def create_server_world_generators(self) -> List[Handler]:\n",
        "        return [\n",
        "            handlers.DefaultWorldGenerator(force_reset=\"true\",\n",
        "                                           generator_options=TREECHOP_WORLD_GENERATOR_OPTIONS\n",
        "                                           )\n",
        "        ]\n",
        "\n",
        "    def create_server_quit_producers(self) -> List[Handler]:\n",
        "        return [\n",
        "            handlers.ServerQuitFromTimeUp(\n",
        "                (TREECHOP_LENGTH * MS_PER_STEP)),\n",
        "            handlers.ServerQuitWhenAnyAgentFinishes()\n",
        "        ]\n",
        "\n",
        "    def create_server_decorators(self) -> List[Handler]:\n",
        "        return []\n",
        "\n",
        "    def create_server_initial_conditions(self) -> List[Handler]:\n",
        "        return [\n",
        "            handlers.TimeInitialCondition(\n",
        "                allow_passage_of_time=False\n",
        "            ),\n",
        "            handlers.SpawningInitialCondition(\n",
        "                allow_spawning=True\n",
        "            )\n",
        "        ]\n",
        "\n",
        "    def determine_success_from_rewards(self, rewards: list) -> bool:\n",
        "        return sum(rewards) >= self.reward_threshold\n",
        "\n",
        "    def is_from_folder(self, folder: str) -> bool:\n",
        "        return folder == 'survivaltreechop'\n",
        "\n",
        "    def get_docstring(self):\n",
        "        return MY_TREECHOP_DOC"
      ],
      "metadata": {
        "id": "b-qQtxaxDMff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "from minerl.herobraine.env_spec import EnvSpec\n",
        "\n",
        "#MINERL_MY_TEST_V0 = MyTestlEnvSpec_2()\n",
        "MINERL_MY_TREECHOP_V0 = MyTreechop()\n",
        "\n",
        "# Register the envs.\n",
        "ENVS = [env for env in locals().values() if isinstance(env, EnvSpec)]\n",
        "for env in ENVS:\n",
        "    if env.name not in gym.envs.registry.env_specs:\n",
        "        env.register()"
      ],
      "metadata": {
        "id": "HECZM3q2DOga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Network"
      ],
      "metadata": {
        "id": "NdscuqrTDQb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Cnn(nn.Module):\n",
        "    def __init__(self, input_shape=(3, 64, 64)):\n",
        "        super().__init__()\n",
        "        n_input_channels = input_shape[0]\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.flat_size = 1024\n",
        "        self.camera = nn.Linear(512, 5)\n",
        "        self.buttons = nn.Linear(512, 6)\n",
        "\n",
        "    def forward(self, observations):\n",
        "        cnn_output = self.cnn(observations)\n",
        "\n",
        "        camera_output = self.camera(cnn_output)\n",
        "        buttons_ouput = self.buttons(cnn_output)\n",
        "\n",
        "        return camera_output, buttons_ouput\n",
        "\n",
        "    def initial_hidden_state(self):\n",
        "        h0 = torch.zeros(2, 1, 512).to(device)\n",
        "        c0 = torch.zeros(2, 1, 512).to(device)\n",
        "        hidden = (h0, c0)\n",
        "        return hidden\n",
        "\n",
        "    def preprocess(self, img, img_size=(64, 64)):\n",
        "        pov = cv2.resize(img, img_size).astype(np.float32)\n",
        "        state = pov.transpose(2, 0, 1) / 255.0\n",
        "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "        return state\n",
        "\n",
        "    def get_result(self, camera_logits, button_logits, t=1.2):\n",
        "        camera_probabilities = torch.softmax(camera_logits/t, dim=1)[0].detach().cpu().numpy()\n",
        "        button_probabilities = torch.softmax(button_logits/t, dim=1)[0].detach().cpu().numpy()\n",
        "\n",
        "        camera_action = np.random.choice(5, p=camera_probabilities)\n",
        "        button_action = np.random.choice(6, p=camera_probabilities)\n",
        "\n",
        "        return camera_action, button_action\n"
      ],
      "metadata": {
        "id": "HxCDRpvoDSH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_shape, output_dim):\n",
        "        super().__init__()\n",
        "        n_input_channels = input_shape[0]\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, observations):\n",
        "        return self.cnn(observations)"
      ],
      "metadata": {
        "id": "I9_vcLxHDUqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Action"
      ],
      "metadata": {
        "id": "FkHOQzv8DZDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_action_to_agent(dataset_actions, camera_margin=5):\n",
        "\n",
        "    camera_actions = dataset_actions[\"camera\"].squeeze()\n",
        "    #attack_actions = dataset_actions[\"attack\"].squeeze()\n",
        "    forward_actions = dataset_actions[\"forward\"].squeeze()\n",
        "    left_actions = dataset_actions[\"left\"].squeeze()\n",
        "    right_actions = dataset_actions[\"right\"].squeeze()\n",
        "    jump_actions = dataset_actions[\"jump\"].squeeze()\n",
        "    sprint_actions = dataset_actions[\"sprint\"].squeeze()\n",
        "\n",
        "    batch_size = len(camera_actions)\n",
        "    ca = np.zeros((batch_size,), dtype=int)\n",
        "    ba = np.zeros((batch_size,), dtype=int)\n",
        "\n",
        "    for i in range(len(camera_actions)):\n",
        "        if camera_actions[i][0] < -camera_margin:\n",
        "            ca[i] = 1\n",
        "        elif camera_actions[i][0] > camera_margin:\n",
        "            ca[i] = 2\n",
        "        elif camera_actions[i][1] > camera_margin:\n",
        "            ca[i] = 3\n",
        "        elif camera_actions[i][1] < -camera_margin:\n",
        "            ca[i] = 4\n",
        "        else:\n",
        "            ca[i] = 0\n",
        "\n",
        "        if jump_actions[i] == 1:\n",
        "            ba[i] = 3\n",
        "        elif left_actions[i] == 1:\n",
        "            ba[i] = 1\n",
        "        elif right_actions[i] == 1:\n",
        "            ba[i] = 2\n",
        "        elif sprint_actions[i] == 1:\n",
        "            ba[i] = 4\n",
        "        else:\n",
        "            ba[i] = 0\n",
        "\n",
        "    return ca, ba\n",
        "\n",
        "class ActionShaping():\n",
        "    def __init__(self, env, camera_angle=10):\n",
        "        self.env = env\n",
        "        self.camera_angle = camera_angle\n",
        "        self._button_actions = [\n",
        "            dict(forward=1),\n",
        "            # dict(back=1),\n",
        "            dict(left=1),\n",
        "            dict(right=1),\n",
        "            dict(forward=1, jump=1),\n",
        "            dict(forward=1, sprint=1),\n",
        "        ]\n",
        "        self._camera_actions = [\n",
        "            dict(),\n",
        "            dict(camera=[-self.camera_angle, 0]),\n",
        "            dict(camera=[self.camera_angle, 0]),\n",
        "            dict(camera=[0, self.camera_angle]),\n",
        "            dict(camera=[0, -self.camera_angle]),\n",
        "        ]\n",
        "\n",
        "        self.min_distance = None\n",
        "        self.last_distance = None\n",
        "\n",
        "\n",
        "    def get_action(self, ca, ba, t=1.2):\n",
        "        ca_probabilities = torch.softmax(ca/t, dim=1)[0].detach().cpu().numpy()\n",
        "        ba_probabilities = torch.softmax(ba/t, dim=1)[0].detach().cpu().numpy()\n",
        "        # Sample action according to the probabilities\n",
        "        camera_action = np.random.choice(self._camera_actions, p=ca_probabilities)\n",
        "        button_action = np.random.choice(self._button_actions, p=ca_probabilities)\n",
        "\n",
        "        act = self.env.action_space.noop()\n",
        "        act.update(camera_action)\n",
        "        act.update(button_action)\n",
        "        # act.update(dict(jump=1))\n",
        "\n",
        "        ci = self._camera_actions.index(camera_action)\n",
        "        bi = self._button_actions.index(button_action)\n",
        "\n",
        "        return act, ci, bi\n",
        "\n",
        "    def get_reward(self, obs, destination=[-205, 258]):\n",
        "        location = np.array([obs['location_stats']['xpos'].item(), obs['location_stats']['zpos'].item()])\n",
        "        destination = np.array(destination)\n",
        "        distance = np.sqrt(np.sum((location - destination) ** 2))\n",
        "        if self.min_distance == None:\n",
        "            self.min_distance = distance\n",
        "            self.last_distance = distance\n",
        "            return 0\n",
        "\n",
        "        if self.min_distance - distance > 0.1:\n",
        "            self.min_distance = distance\n",
        "            self.last_distance = distance\n",
        "            return 5\n",
        "        elif distance < 10:\n",
        "            self.min_distance = None\n",
        "            self.last_distance = None\n",
        "            return 100\n",
        "        elif distance - self.last_distance > 0.1:\n",
        "            self.last_distance = distance\n",
        "            return -0.5\n",
        "        else:\n",
        "            self.last_distance = distance\n",
        "            return 0\n",
        "\n",
        "    def reset(self, seed=10):\n",
        "        self.env.seed(seed)\n",
        "        self.min_distance = None\n",
        "        self.now_distance = None\n",
        "        obs = self.env.reset()\n",
        "        return obs\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def action(self, ac, t=1.2):\n",
        "        probabilities = torch.softmax(ac/t, dim=1)[0].detach().cpu().numpy()\n",
        "        action = np.random.choice(self._actions, p=probabilities)\n",
        "        act = self.env.action_space.noop()\n",
        "        act.update(action)\n",
        "        return act\n"
      ],
      "metadata": {
        "id": "68D4skFYDVAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActionShaping_2(gym.ActionWrapper):\n",
        "    def __init__(self, env, camera_angle=10):\n",
        "        super().__init__(env)\n",
        "        self.camera_angle = camera_angle\n",
        "        self._actions = [\n",
        "            [('attack', 1)],\n",
        "            [('forward', 1)],\n",
        "            [('jump', 1)],\n",
        "            [('camera', [-self.camera_angle, 0])],\n",
        "            [('camera', [self.camera_angle, 0])],\n",
        "            [('camera', [0, self.camera_angle])],\n",
        "            [('camera', [0, -self.camera_angle])],\n",
        "        ]\n",
        "        self.actions = []\n",
        "        for actions in self._actions:\n",
        "            act = self.env.action_space.noop()\n",
        "            for a, v in actions:\n",
        "                act[a] = v\n",
        "                act['attack'] = 1\n",
        "            self.actions.append(act)\n",
        "        self.action_space = gym.spaces.Discrete(len(self.actions))\n",
        "\n",
        "    def action(self, action):\n",
        "        return self.actions[action]"
      ],
      "metadata": {
        "id": "IUeJ3mAZDbkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-FJ3kLo5DeL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets"
      ],
      "metadata": {
        "id": "bMI-SUzbDevL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get data, find datasets in https://minerl.readthedocs.io/en/v0.4.4/environments/index.html.\n",
        "minerl.data.download(directory='data', environment='MineRLNavigate-v0')\n",
        "data = minerl.data.make(\"MineRLNavigate-v0\", data_dir='data', num_workers=2)"
      ],
      "metadata": {
        "id": "CZmCElgRDgjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Behavior cloning"
      ],
      "metadata": {
        "id": "Qd5FkToZDjZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "model = Cnn().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "AUMtZvp0Dh91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "step = 0\n",
        "losses = []\n",
        "for state, action, _, _, _ \\\n",
        "          in tqdm(data.batch_iter(num_epochs=6, batch_size=32, seq_len=1)):\n",
        "    # Get pov observations\n",
        "    obs = state['pov'].squeeze().astype(np.float32)\n",
        "    # Transpose and normalize\n",
        "    obs = obs.transpose(0, 3, 1, 2) / 255.0\n",
        "\n",
        "    # Translate batch of actions for the ActionShaping wrapper\n",
        "    ca, ba = dataset_action_to_agent(action)\n",
        "    # Remove samples with no corresponding action\n",
        "    mask = ca != -1\n",
        "    obs = obs[mask]\n",
        "    ca = ca[mask]\n",
        "    ba = ba[mask]\n",
        "\n",
        "    # Update weights with backprop\n",
        "    camera_output, buttons_ouput = model(torch.from_numpy(obs).float().to(device))\n",
        "    camera_loss = criterion(camera_output, torch.from_numpy(ca).long().to(device))\n",
        "    buttons_loss = criterion(buttons_ouput, torch.from_numpy(ba).long().to(device))\n",
        "    total_loss = camera_loss + buttons_loss\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print loss\n",
        "    step += 1\n",
        "    losses.append(total_loss.item())\n",
        "    if (step % 2000) == 0:\n",
        "        mean_loss = sum(losses) / len(losses)\n",
        "        tqdm.write(f'Step {step:>5} | Training loss = {mean_loss:.3f}')\n",
        "        losses.clear()\n",
        "\n",
        "    # break\n",
        "\n",
        "torch.save(model.state_dict(), 'navigation_1.pth')\n",
        "del data"
      ],
      "metadata": {
        "id": "ersMCvnwDxTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FsSBBNv0Dzck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Expanded Knowledge Distillation"
      ],
      "metadata": {
        "id": "BiXr0PEhD0zP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VPT\n",
        "%%capture\n",
        "!pip3 install gym3\n",
        "!git clone https://github.com/openai/video-pre-training\n",
        "%cd video-pre-training"
      ],
      "metadata": {
        "id": "r9J1nMACD7QA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "weights_file = \"https://openaipublic.blob.core.windows.net/minecraft-rl/models/rl-from-early-game-2x.weights\"  #@param {type: \"string\", allow-input:true} [\"https://openaipublic.blob.core.windows.net/minecraft-rl/models/foundation-model-1x.weights\", \"https://openaipublic.blob.core.windows.net/minecraft-rl/models/foundation-model-2x.weights\", \"https://openaipublic.blob.core.windows.net/minecraft-rl/models/foundation-model-1x.weights\", \"https://openaipublic.blob.core.windows.net/minecraft-rl/models/bc-house-3x.weights\", \"https://openaipublic.blob.core.windows.net/minecraft-rl/models/bc-early-game-2x.weights\", \"https://openaipublic.blob.core.windows.net/minecraft-rl/models/bc-early-game-3x.weights\", \"https://openaipublic.blob.core.windows.net/minecraft-rl/models/rl-from-foundation-2x.weights\", \"https://openaipublic.blob.core.windows.net/minecraft-rl/models/rl-from-house-2x.weights\", \"https://openaipublic.blob.core.windows.net/minecraft-rl/models/rl-from-early-game-2x.weights\"]\n",
        "multiplier = [x for x in [\"1x\", \"2x\", \"3x\"] if x in weights_file][0]\n",
        "!wget {weights_file} -O model\n",
        "!wget https://openaipublic.blob.core.windows.net/minecraft-rl/models/{multiplier}.model -O model\n",
        "!wget {weights_file} -O weights"
      ],
      "metadata": {
        "id": "crmCW2FnECZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd video-pre-training\n",
        "\n",
        "from agent import MineRLAgent\n",
        "import pickle\n",
        "import torch\n",
        "from lib.policy import MinecraftAgentPolicy\n",
        "from lib.tree_util import tree_map\n",
        "from lib.action_mapping import CameraHierarchicalMapping\n",
        "from gym3.types import DictType\n",
        "\n",
        "class VPTAgent(MineRLAgent):\n",
        "    def __init__(self, env, device=None, policy_kwargs=None, pi_head_kwargs=None) -> None:\n",
        "        super().__init__(env, device=None, policy_kwargs=None, pi_head_kwargs=None)\n",
        "\n",
        "    def get_action(self, minerl_obs):\n",
        "        agent_input = self._env_obs_to_agent(minerl_obs)\n",
        "\n",
        "        agent_action, self.hidden_state, result = self.policy.act(\n",
        "            agent_input, self._dummy_first, self.hidden_state,\n",
        "            stochastic=True, return_pd=True\n",
        "        )\n",
        "        minerl_action = self._agent_action_to_env(agent_action)\n",
        "\n",
        "        # actions = {'buttons': result[\"pd\"]['buttons'].squeeze(0).max(1)[1].view(1, 1), 'camera': result[\"pd\"]['camera'].squeeze(0).max(1)[1].view(1, 1)}\n",
        "        # minerl_action = super()._agent_action_to_env(actions)\n",
        "\n",
        "        return minerl_action, agent_action, result\n",
        "\n",
        "    def preprocess(self, obs, shape=(64, 64)):\n",
        "        pov = obs['pov']\n",
        "        pov = cv2.resize(pov, shape).astype(np.float32)\n",
        "        state = pov.transpose(2, 0, 1) / 255.0\n",
        "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "        return state\n",
        "\n",
        "    def net_action(state, model):\n",
        "        with torch.no_grad():\n",
        "            # t.max(1) will return the largest column value of each row.\n",
        "            # second column on max result is index of where max element was\n",
        "            # found, so we pick action with the larger expected reward.\n",
        "            return model(state)[0].max(1)[1].view(1, 1), model(state)[1].max(1)[1].view(1, 1)\n",
        "\n",
        "    def get_agent_action(self, obs, model, hidden, deterministic=False, preprocess_shape=(64, 64)):\n",
        "        state = self.preprocess(obs, preprocess_shape)\n",
        "        camera_output, buttons_ouput, hidden_new  = model(state, hidden)\n",
        "        # buttons_ouput = buttons_ouput.max(1)[1].view(1, 1)\n",
        "        camera_output = camera_output.max(1)[1].view(1, 1)\n",
        "        buttons_ouput = self.sample(buttons_ouput.unsqueeze(0), deterministic)\n",
        "        # camera_output = self.sample(camera_output.unsqueeze(0), deterministic)\n",
        "        actions = {'buttons': buttons_ouput, 'camera': camera_output}\n",
        "        # print(actions)\n",
        "        minerl_action = super()._agent_action_to_env(actions)\n",
        "\n",
        "        return minerl_action, hidden_new\n",
        "\n",
        "    def get_agent_output(self, obs, model, hidden):\n",
        "        state = self.preprocess(obs)\n",
        "        camera_output, buttons_ouput, hidden_new  = model(state, hidden)\n",
        "        actions = {'buttons': buttons_ouput, 'camera': camera_output}\n",
        "\n",
        "        return actions, hidden_new\n",
        "\n",
        "    def sample(self, logits: torch.Tensor, deterministic: bool = False):\n",
        "        if deterministic:\n",
        "            return torch.argmax(logits, dim=-1)\n",
        "        else:\n",
        "            # Gumbel-Softmax trick.\n",
        "            u = torch.rand_like(logits)\n",
        "            # In float16, if you have around 2^{float_mantissa_bits} logits, sometimes you'll sample 1.0\n",
        "            # Then the log(-log(1.0)) will give -inf when it should give +inf\n",
        "            # This is a silly hack to get around that.\n",
        "            # This hack does not skew the probability distribution, because this event can't possibly win the argmax.\n",
        "            u[u == 1.0] = 0.999\n",
        "\n",
        "            return torch.argmax(logits - torch.log(-torch.log(u)), dim=-1)\n",
        "\n",
        "    def gaussian_sample(self, pd_params: torch.Tensor, deterministic: bool = False) -> torch.Tensor:\n",
        "        means = pd_params[..., 0]\n",
        "        log_std = pd_params[..., 1]\n",
        "\n",
        "        if deterministic:\n",
        "            return means\n",
        "        else:\n",
        "            return torch.randn_like(means) * torch.exp(log_std) + means\n"
      ],
      "metadata": {
        "id": "_LvvM1N0EGh5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eecd19b-aff3-4e37-a406-41343608ec31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'video-pre-training'\n",
            "/content/video-pre-training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_parameters = pickle.load(open(\"model\", \"rb\"))\n",
        "policy_kwargs = agent_parameters[\"model\"][\"args\"][\"net\"][\"args\"]\n",
        "pi_head_kwargs = agent_parameters[\"model\"][\"args\"][\"pi_head_opts\"]\n",
        "pi_head_kwargs[\"temperature\"] = float(pi_head_kwargs[\"temperature\"])"
      ],
      "metadata": {
        "id": "chUoi-2tEIIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialization environment, needs MineRL v1.0\n",
        "env = gym.make('MyMineRLTreechop-v0')\n",
        "env.seed(10)\n",
        "obs = env.reset()\n",
        "plt.imshow(obs['pov'])"
      ],
      "metadata": {
        "id": "do3AfXeFEQFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm.auto import trange\n",
        "\n",
        "wooden_pickaxe_dataset = []\n",
        "\n",
        "while len(wooden_pickaxe_dataset) < 12:\n",
        "    print(len(wooden_pickaxe_dataset))\n",
        "    env = gym.make('MyMineRLTreechop-v0')\n",
        "    env.seed(10)\n",
        "    obs = env.reset()\n",
        "    agent = VPTAgent(env, policy_kwargs=policy_kwargs, pi_head_kwargs=pi_head_kwargs, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    agent.load_weights(\"weights\")\n",
        "    record = []\n",
        "    actions = []\n",
        "    play_steps = 1000\n",
        "    live_display = False\n",
        "    try:\n",
        "        for _ in trange(play_steps):  # The t part will get erased anyway\n",
        "            minerl_action, agent_action, result = agent.get_action(obs)\n",
        "            obs, reward, done, info = env.step(minerl_action)\n",
        "\n",
        "            #if np.mean(cv2.cvtColor(obs['pov'][0:99], cv2.COLOR_BGR2GRAY)) < 40:\n",
        "            record.append(obs['pov'])\n",
        "            actions.append(result['pd'])\n",
        "\n",
        "            if obs['equipped_items']['mainhand']['type'] == 'wooden_pickaxe':\n",
        "                print('Agent got the wooden_pickaxe!')\n",
        "                wooden_pickaxe_dataset.append({'pov': record, 'pd': actions})\n",
        "                done = True\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            if live_display:\n",
        "                clear_output(wait=True)\n",
        "                plt.axis(\"off\")\n",
        "                plt.imshow(obs[\"pov\"])\n",
        "                plt.show()\n",
        "    except KeyboardInterrupt:\n",
        "        pass"
      ],
      "metadata": {
        "id": "tot94XLdEe3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save POVs and actions as .npz file\n",
        "dataset_array = np.array(wooden_pickaxe_dataset)\n",
        "np.savez('dataset.npz', dataset=dataset_array)"
      ],
      "metadata": {
        "id": "ecU2cVDGZV17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "O7kqHGP3vBwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path, index):\n",
        "    data = np.load('dataset.npz')\n",
        "    dataset_array = data['dataset']\n",
        "    pov = dataset_array[index]['pov']\n",
        "    actions = dataset_array[index]['pd']\n",
        "\n",
        "    return pov, actions"
      ],
      "metadata": {
        "id": "_5T2i067cq13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student_model = Cnn().to(device)\n",
        "from IPython.display import clear_output\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm.auto import trange\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(student_model.parameters(), lr=0.0001)\n",
        "losses = []\n",
        "\n",
        "record= []\n",
        "play_steps = 1000\n",
        "episodes = 10\n",
        "\n",
        "try:\n",
        "    for i in range(episodes):\n",
        "        env.seed(10)\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        teacher_model = VPTAgent(env, policy_kwargs=policy_kwargs, pi_head_kwargs=pi_head_kwargs, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        teacher_model.load_weights(\"weights\")\n",
        "\n",
        "        h0 = torch.zeros(2, 1, 512).to(device)\n",
        "        c0 = torch.zeros(2, 1, 512).to(device)\n",
        "        hidden = (h0, c0)\n",
        "\n",
        "        print('Env reset done. Episode: ' + str(i))\n",
        "\n",
        "        for _ in trange(play_steps):  # The t part will get erased anyway\n",
        "            minerl_action, agent_action, result = teacher_model.get_action(obs)\n",
        "\n",
        "            actions, hidden_new = teacher_model.get_agent_output(obs, student_model, hidden)\n",
        "            hidden_new = (hidden_new[0].detach(), hidden_new[1].detach())\n",
        "            hidden = hidden_new\n",
        "\n",
        "            # Update weights with backprop\n",
        "\n",
        "            loss_buttons = policy_distillation_loss(result['pd']['buttons'].squeeze(0), actions['buttons'], temperature=1.5)\n",
        "            loss_camera = policy_distillation_loss(result['pd']['camera'].squeeze(0), actions['camera'], temperature=1.5)\n",
        "\n",
        "            total_loss = loss_buttons + loss_camera\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            losses.append(total_loss.item())\n",
        "\n",
        "            # Next step\n",
        "            obs, reward, done, info = env.step(minerl_action)\n",
        "            #record.append(obs)\n",
        "\n",
        "            if obs['equipped_items']['mainhand']['type'] == 'wooden_pickaxe':\n",
        "                print('Agent got the wooden_pickaxe!')\n",
        "                done = True\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        mean_loss = sum(losses) / len(losses)\n",
        "        tqdm.write(f'Training loss = {mean_loss:.3f}')\n",
        "        losses.clear()\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    pass"
      ],
      "metadata": {
        "id": "hv-SlWelEiMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def action_step(action):\n",
        "  ac = env.action_space.noop()\n",
        "  ac.update(action)\n",
        "  obs, reward, done, info = env.step(ac)\n",
        "  plt.imshow(obs[\"pov\"])\n",
        "  plt.show()\n",
        "\n",
        "def policy_distillation_loss(outputs_teacher, outputs_student, temperature):\n",
        "    # KL loss\n",
        "    KD_loss = F.kl_div(\n",
        "        F.log_softmax(outputs_student/temperature, dim=1),\n",
        "        F.softmax(outputs_teacher/temperature, dim=1),\n",
        "        reduction='batchmean') * temperature * temperature\n",
        "\n",
        "    return KD_loss"
      ],
      "metadata": {
        "id": "avC985OcEu7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wdlz5MQ1ExG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tune"
      ],
      "metadata": {
        "id": "O9hVPL8xExbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_net = Cnn().to(device)\n",
        "\n",
        "policy_net.load_state_dict(torch.load('/content/navigation_2.pth'))\n",
        "policy_net.train()\n",
        "\n",
        "target_net = Cnn().to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.train()"
      ],
      "metadata": {
        "id": "4H5H5GUHE1IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple, deque\n",
        "import random\n",
        "\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'camera_action', 'button_action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "upjldtg-E2ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "GAMMA = 0.99\n",
        "LR = 1e-4\n",
        "\n",
        "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return 0\n",
        "\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    batch = Transition(*zip(*transitions))\n",
        "    #print(batch.state)\n",
        "\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    #print(state_batch.shape)\n",
        "    camera_action_batch = torch.cat(batch.camera_action)\n",
        "    button_action_batch = torch.cat(batch.button_action)\n",
        "    camera_action_batch =  torch.unsqueeze(camera_action_batch, 0)\n",
        "    button_action_batch =  torch.unsqueeze(button_action_batch, 0)\n",
        "    #action_batch =  torch.unsqueeze(action_batch, 0)\n",
        "    #print(action_batch.shape)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Obtain Q values for both camera and buttons from the policy net\n",
        "    camera_state_action_values, buttons_state_action_values = policy_net(state_batch)\n",
        "    camera_state_action_values = camera_state_action_values.gather(1, camera_action_batch)\n",
        "    buttons_state_action_values = buttons_state_action_values.gather(1, button_action_batch)\n",
        "\n",
        "    # Obtain next state Q values for both camera and buttons from the target net\n",
        "    with torch.no_grad():\n",
        "        camera_next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "        buttons_next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "        camera_next_state, buttons_next_state = target_net(non_final_next_states)\n",
        "        camera_next_state_values[non_final_mask] = camera_next_state.max(1)[0]\n",
        "        buttons_next_state_values[non_final_mask] = buttons_next_state.max(1)[0]\n",
        "\n",
        "    # Compute the expected Q values\n",
        "    expected_camera_state_action_values = (camera_next_state_values * GAMMA) + reward_batch\n",
        "    expected_buttons_state_action_values = (buttons_next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute Huber loss\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    camera_loss = criterion(camera_state_action_values, expected_camera_state_action_values.unsqueeze(1))\n",
        "    buttons_loss = criterion(buttons_state_action_values, expected_buttons_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Total loss is sum of individual losses\n",
        "    loss = camera_loss + buttons_loss\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # In-place gradient clipping\n",
        "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "ify0_VkME3pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import count\n",
        "\n",
        "TAU = 0.005\n",
        "\n",
        "replay = deque(maxlen=10000)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    num_episodes = 10\n",
        "else:\n",
        "    num_episodes = 2\n",
        "\n",
        "steps = 0\n",
        "total_rewards = [0]\n",
        "play_steps = 6000\n",
        "T = 100\n",
        "losses = []\n",
        "\n",
        "env = gym.make('MyMineRLTreechop-v0')\n",
        "env.seed(10)\n",
        "env_action = ActionShaping(env)\n",
        "\n",
        "for i_episode in range(num_episodes):\n",
        "    print('Episode: ' + str(i_episode), end='')\n",
        "    # Initialize the environment and get it's state\n",
        "\n",
        "    obs = env_action.reset()\n",
        "    pov = obs[\"pov\"]\n",
        "    pov = cv2.resize(pov, (64, 64)).astype(np.float32)\n",
        "    state = pov.transpose(2, 0, 1) / 255.0\n",
        "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    best_distance= 0\n",
        "    print(' reset')\n",
        "    for _ in range(play_steps):\n",
        "\n",
        "        camera_output, buttons_ouput = policy_net(state)\n",
        "        minerl_action, ci, bi = env_action.get_action(camera_output, buttons_ouput, t=T)\n",
        "        obs, reward, done, info = env_action.step(minerl_action)\n",
        "        steps += 1\n",
        "\n",
        "        if T > 1.2:\n",
        "            T = max(T * 0.9995, 1.2)\n",
        "\n",
        "        reward = env_action.get_reward(obs)\n",
        "        # if reward > 0:\n",
        "        #     print(reward)\n",
        "        # print(env_action.distance)\n",
        "\n",
        "        total_rewards.append(reward+total_rewards[-1])\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "        if reward == 100:\n",
        "            done = True\n",
        "\n",
        "        if done:\n",
        "            next_state = None\n",
        "        else:\n",
        "            pov = obs[\"pov\"]\n",
        "            replay.append(pov)\n",
        "            pov = cv2.resize(pov, (64, 64)).astype(np.float32)\n",
        "            next_state = pov.transpose(2, 0, 1) / 255.0\n",
        "            next_state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "        # Store the transition in memory\n",
        "        camera_action = torch.tensor([ci], device=device)\n",
        "        button_action = torch.tensor([bi], device=device)\n",
        "        memory.push(state, camera_action, button_action, next_state, reward)\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "        # Perform one step of the optimization (on the policy network)\n",
        "        loss = optimize_model()\n",
        "        losses.append(loss)\n",
        "\n",
        "        # Soft update of the target network's weights\n",
        "        # θ′ ← τ θ + (1 −τ )θ′\n",
        "        target_net_state_dict = target_net.state_dict()\n",
        "        policy_net_state_dict = policy_net.state_dict()\n",
        "        for key in policy_net_state_dict:\n",
        "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
        "        target_net.load_state_dict(target_net_state_dict)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        if steps % 1000 == 0:\n",
        "            mean_loss = sum(losses) / len(losses)\n",
        "            print('Step: ' + str(steps) + ' | Training loss = '+ str(mean_loss))\n",
        "            losses.clear()\n",
        "\n",
        "        # break\n",
        "\n",
        "    print('Best distance: ' + str(env_action.min_distance))\n",
        "\n",
        "print('Complete')"
      ],
      "metadata": {
        "id": "ZkQ3eIgyE5wG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(total_rewards)"
      ],
      "metadata": {
        "id": "RGH14-xAFANq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(policy_net.state_dict(), 'navigation_3.pth')"
      ],
      "metadata": {
        "id": "9ca32aSfE832"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "qD5gnhhqFBK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = [0]\n",
        "env_action = ActionShaping_2(env)\n",
        "action_list = np.arange(env_action.action_space.n)\n",
        "record = []\n",
        "log_number = 4\n",
        "\n",
        "for step in tqdm(range(3000)):\n",
        "    # Get input in the correct format\n",
        "    obs = torch.from_numpy(cv2.resize(obs['pov'], (64, 64)).transpose(2, 0, 1)[None].astype(np.float32) / 255).cuda()\n",
        "    # Turn logits into probabilities\n",
        "    logits = model(obs)\n",
        "\n",
        "    t = 1.2\n",
        "    probabilities = torch.softmax(logits/t, dim=1)[0].detach().cpu().numpy()\n",
        "    # Sample action according to the probabilities\n",
        "    action = np.random.choice(action_list, p=probabilities)\n",
        "\n",
        "    obs, reward, _, _ = env_action.step(action)\n",
        "    record.append(obs['pov'])\n",
        "\n",
        "    if obs['inventory']['oak_log'].item() > log_number:\n",
        "        log_number = obs['inventory']['oak_log'].item()\n",
        "        print(str(obs['inventory']['oak_log'].item()) + ' oak logs')\n",
        "        rewards.append(rewards[-1] + 5)\n",
        "    else:\n",
        "        rewards.append(rewards[-1] + 0)"
      ],
      "metadata": {
        "id": "FeP7_PKVFG9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "# Save the task process as a video\n",
        "file_path='saveVideo.mp4'\n",
        "size=(320,180)\n",
        "fps = 30\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "videoWriter = cv2.VideoWriter(file_path,fourcc,fps,size)\n",
        "\n",
        "for item in record:\n",
        "    b, g, r = cv2.split(item)\n",
        "    img = cv2.merge([r, g, b])\n",
        "    img = cv2.resize(img, (320, 180))\n",
        "    # img = cv2.imread(item)\n",
        "    videoWriter.write(img)\n",
        "\n",
        "videoWriter.release()"
      ],
      "metadata": {
        "id": "YSafUBsjFaeE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}